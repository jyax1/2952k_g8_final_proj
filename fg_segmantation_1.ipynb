{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# DINOv2\n",
    "Foundation model from META, a Visual Transformer (ViT) pre-trained on a high-quality corpus.\n",
    "Smaller models distilled from the large model are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#@formatter:off\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#@formatter:on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Prepare model\n",
    "\n",
    "There are two options to load the model:\n",
    "**Load model with weights from PyTorch hub** - does not work for CPU, because the model uses attention layer with CUDA-only implementation\n",
    "**Use the model from this repo, with externally downloaded weights** - works after a few tweaks\n",
    "  - in vision_transformer.py, change the attention layer to ordinary Attention\n",
    "  - remove `.cuda()` in `setup.py`\n",
    "Load only the desired model in the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/yilong/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/yilong/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/yilong/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/yilong/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "# BASE\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LARGE\n",
    "conf = load_and_merge_config('eval/vitl14_pretrain')\n",
    "model = build_model_for_eval(conf, '../dinov2_vitl14_pretrain.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GIANT\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Try it out\n",
    "\n",
    "Load a set of training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "images = []\n",
    "\n",
    "for i in range(2):\n",
    "    image = cv2.imread(f\"dino_test_imgs/{i}.jpg\");\n",
    "    image = cv2.resize(image, (448,448))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image.astype('float32')/255\n",
    "    images.append(image)\n",
    "    plt.subplot(220+i)\n",
    "    plt.imshow(image)\n",
    "\n",
    "# We need to reorder the images to [batch, channel, widht, height]\n",
    "# The array of loaded images is [batch, height, width, channel]\n",
    "images_arr = np.stack(images)\n",
    "input_tensor = torch.Tensor(np.transpose(images_arr, [0, 3, 2, 1]))\n",
    "\n",
    "transform = tt.Compose([tt.Normalize(mean=0.5, std=0.2)])\n",
    "\n",
    "input_tensor = transform(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Pass the images through the DINOv2 model, and grab the outputs\n",
    "\n",
    "We can use either the normalized patch tokens, or the full pre-normalized output, they give very similar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = model.forward_features(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patch_tokens = result['x_norm_patchtokens'].detach().numpy().reshape([4,1024,-1])\n",
    "\n",
    "# patch_tokens = result['x_prenorm'].detach().numpy().reshape([4,1025,-1])\n",
    "# patch_tokens = patch_tokens[:,1:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then, we can build a linear model over the patch tokens. We can segment the foreground object by building a simple linear classifier over the patch tokens: we compute the first component of the PCA and scale and threshold the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "fg_pca = PCA(n_components=1)\n",
    "\n",
    "masks=[]\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "all_patches = patch_tokens.reshape([-1,1024])\n",
    "reduced_patches = fg_pca.fit_transform(all_patches)\n",
    "# scale the feature to (0,1)\n",
    "norm_patches = minmax_scale(reduced_patches)\n",
    "\n",
    "# reshape the feature value to the original image size\n",
    "image_norm_patches = norm_patches.reshape([4,1024])\n",
    "\n",
    "for i in range(4):\n",
    "    image_patches = image_norm_patches[i,:]\n",
    "\n",
    "    # choose a threshold to segment the foreground\n",
    "    mask = (image_patches > 0.6).ravel()\n",
    "    masks.append(mask)\n",
    "\n",
    "    image_patches[np.logical_not(mask)] = 0\n",
    "\n",
    "    plt.subplot(221+i)\n",
    "    plt.imshow(images[i])\n",
    "    plt.imshow(image_patches.reshape([32,-1]).T, extent=(0,448,448,0), alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then we can further use a linear model to describe the foreground object. We compute the firs 3 components of PCA of the foreground patches, and visualize the features as RGB colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "object_pca = PCA(n_components=3)\n",
    "\n",
    "# extract foreground patches\n",
    "mask_indices = [0, *np.cumsum([np.sum(m) for m in masks]), -1]\n",
    "fg_patches = np.vstack([patch_tokens[i,masks[i],:] for i in range(4)])\n",
    "\n",
    "# fit PCA to foreground, scale each feature to (0,1)\n",
    "reduced_patches = object_pca.fit_transform(fg_patches)\n",
    "reduced_patches = minmax_scale(reduced_patches)\n",
    "\n",
    "print(object_pca.explained_variance_ratio_)\n",
    "\n",
    "# reshape the features to the original image size\n",
    "plt.figure(figsize=(10,20))\n",
    "for i in range(4):\n",
    "    patch_image = np.zeros((1024,3), dtype='float32')\n",
    "    patch_image[masks[i],:] = reduced_patches[mask_indices[i]:mask_indices[i+1],:]\n",
    "\n",
    "    color_patches = patch_image.reshape([32,-1,3]).transpose([1,0,2])\n",
    "\n",
    "    plt.subplot(421+(2*i))\n",
    "    plt.imshow(color_patches)\n",
    "    plt.subplot(421+(2*i)+1)\n",
    "    plt.imshow(images[i])\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(425+i)\n",
    "    plt.imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Test\n",
    "\n",
    "We try the PCA features on an unknown image of another crane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_image = cv2.imread(f\"./data/crane/crane_test.jpg\")\n",
    "test_image = cv2.resize(test_image, (672,672))  # unicorn: 224,280\n",
    "test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "test_image = test_image.astype('float32')/255\n",
    "\n",
    "plt.imshow(test_image)\n",
    "\n",
    "test_images = [test_image]\n",
    "\n",
    "test_images_arr = np.stack(test_images)\n",
    "test_tensor = torch.Tensor(np.transpose(test_images_arr, [0, 3, 2, 1]))\n",
    "test_tensor = transform(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_result = model.forward_features(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_patch_tokens = test_result['x_norm_patchtokens'].detach().numpy().reshape([2304, -1])\n",
    "fg_result = fg_pca.transform(test_patch_tokens)\n",
    "fg_result = minmax_scale(fg_result)\n",
    "\n",
    "fg_mask = (fg_result > 0.5)\n",
    "\n",
    "object_result = object_pca.transform(test_patch_tokens)\n",
    "object_result = minmax_scale(object_result)\n",
    "\n",
    "only_object = np.zeros_like(object_result)\n",
    "only_object[fg_mask.ravel(), :] = object_result[fg_mask.ravel(), :]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(121)\n",
    "plt.imshow(test_image)\n",
    "plt.subplot(122)\n",
    "plt.imshow(only_object.reshape([48, -1, 3]).transpose([1, 0, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "These results are very impressive, the model generalized from 4 images with only a linear model built on top.\n",
    "The results may be biased because we used readily available images from the internet/wikipedia, that were probably part of the pre-training. We should test it with some custom images later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
